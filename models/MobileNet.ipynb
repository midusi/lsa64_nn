{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TimeDistributedConv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "znuik6QlSIlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44d714d-3891-435b-91cd-aa1124872bca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owHW6xBk6XG0"
      },
      "source": [
        "CALLBACKS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE5BbQBK4FJe"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "class MyCheckpoint(Callback):\n",
        "  def __init__(self, start_epoch, exp_folder, weights_path, pickles_path, EXP_NAME, model_name):\n",
        "    super(MyCheckpoint, self).__init__()\n",
        "    self.current_epoch = start_epoch + 1  \n",
        "    self.timestamps = {}\n",
        "    self.exp_folder = exp_folder\n",
        "    self.weights_path = weights_path\n",
        "    self.pickles_path = pickles_path\n",
        "    self.EXP_NAME = EXP_NAME  \n",
        "    self.model_name = model_name\n",
        "    self.model_training_time = 0\n",
        "    self.start_batch = 0\n",
        "    self.end_batch = 0\n",
        "\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    self.timestamps[self.current_epoch] = {}   \n",
        "    self.timestamps[self.current_epoch][\"start\"] = datetime.now().time().strftime(\"%H:%M\")  \n",
        "\n",
        "  def on_test_begin(self, logs=None):    \n",
        "    self.model.save_weights(os.path.join(self.weights_path+'{}_epoch_{}.h5'.format(self.model_name,self.current_epoch)))          \n",
        "    self.timestamps[self.current_epoch][\"end\"] = datetime.now().time().strftime(\"%H:%M\")  # PASAR ARRIBA POR EL SAVE\n",
        "    self.timestamps[self.current_epoch][\"train_time\"] = self.model_training_time \n",
        "    with open(os.path.join(self.exp_folder, f\"timestamps_epoch{self.current_epoch}.pickle\"), 'wb') as fd:\n",
        "      pickle.dump(self.timestamps, fd)\n",
        "    self.model_training_time = 0\n",
        "\n",
        "  def on_train_batch_begin(self, batch, logs=None):\n",
        "      self.start_batch = time.time()\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "      self.end_batch = time.time()\n",
        "      self.model_training_time+= (self.end_batch - self.start_batch)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):            \n",
        "    with open(os.path.join(self.pickles_path+'{}_epoch_{}.pickle'.format(self.model_name,self.current_epoch)), 'wb') as fd:\n",
        "      pickle.dump(logs, fd)\n",
        "    self.current_epoch = self.current_epoch + 1    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEK9sFcq8-fG"
      },
      "source": [
        "FOLD_NAME = '2fold'\n",
        "DATASET = 'Multisampling32'\n",
        "MODEL_NAME = 'rgb128_'+FOLD_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfDAeLcT0-K3"
      },
      "source": [
        "import os\n",
        "\n",
        "from tensorflow.keras.initializers import HeNormal, GlorotNormal\n",
        "from tensorflow.keras.regularizers import l1_l2, l1\n",
        "from tensorflow.keras.activations import elu\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "\n",
        "basePath = \"/content/gdrive/My Drive/Inform√°tica/Tesina/\"\n",
        "\n",
        "datasets = {            \n",
        "            \"Multisampling32\":{\"path\": basePath+\"Videos/Multisampling32/\",\"num_frames\": 32,\"files_posfix\":\"Multisampling32\"},  \n",
        "            \"Multisampling16\":{\"path\": basePath+\"Videos/Multisampling16/\",\"num_frames\": 16, \"files_posfix\":\"Multisampling16\"},  \n",
        "            \"Multisampling8\":{\"path\": basePath+\"Videos/Multisampling8/\",\"num_frames\": 8, \"files_posfix\":\"Multisampling8\"},  \n",
        "            \"SimpleSampling32\":{\"path\": basePath+\"Videos/SimpleSampling32/\",\"num_frames\": 32, \"files_posfix\":\"SimpleSampling32\"},              \n",
        "  }\n",
        "\n",
        "\n",
        "class Experiment():\n",
        "\n",
        "  def __init__(self, dataset, model_name, fold):\n",
        "    #Logging and checkpoints\n",
        "    self.dataset = dataset\n",
        "    self.model_name = model_name\n",
        "    self.exp_folder = os.path.join(basePath+\"Experimentos/MobileNet+Conv\", dataset, model_name)\n",
        "    self.weights_path = os.path.join(self.exp_folder,\"Weights/\")\n",
        "    self.pickles_path = os.path.join(self.exp_folder,\"Pickles/\")\n",
        "    if (os.path.exists(self.exp_folder)):           \n",
        "      weights = os.listdir(self.weights_path)\n",
        "      # get the latest epoch trained\n",
        "      try:\n",
        "        self.current_epoch = sorted(list(map(lambda x: int(x.split('_')[-1].split('.')[0]), weights)), reverse=True)[0] \n",
        "      except (TypeError, IndexError):\n",
        "        self.current_epoch = 0      \n",
        "    else:      \n",
        "      os.mkdir(self.exp_folder)      \n",
        "      os.mkdir(self.weights_path)\n",
        "      os.mkdir(self.pickles_path)\n",
        "      self.current_epoch = 0\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=os.path.join(self.exp_folder, 'logs'), histogram_freq=1)\n",
        "    early = EarlyStopping(monitor='val_accuracy', patience=4)\n",
        "    self.testing = False\n",
        "    #Data specs\n",
        "    data_dict = datasets[dataset]\n",
        "    self.fold_name = fold\n",
        "    self.files_posfix = data_dict['files_posfix']\n",
        "    self.names_path = os.path.join(basePath+\"/DatasetsNames\", data_dict['files_posfix'])\n",
        "    self.videos_path = data_dict['path'] \n",
        "    self.num_frames = data_dict['num_frames']\n",
        "    self.height = 128 #data_dict['height']\n",
        "    self.width = 128 #data_dict['width']\n",
        "    self.channels = 3\n",
        "    #Model specs\n",
        "    self.activation = elu\n",
        "    self.initializer = GlorotNormal\n",
        "    self.regularizer = None#l1(1e-4)\n",
        "\n",
        "    self.layers = [\n",
        "                        {'units':128},\n",
        "                        {'units':128},\n",
        "                        \n",
        "                    ]\n",
        "\n",
        "    self.dense_layer = 128\n",
        "    self.batch_size = 32\n",
        "    self.epochs_to_train = 48 - self.current_epoch\n",
        "    self.lr = 0.001\n",
        "    self.callbacks = [MyCheckpoint(self.current_epoch, self.exp_folder, self.weights_path, self.pickles_path, self.dataset, self.model_name), tensorboard, early]\n",
        "\n",
        "  def save_resume(self):\n",
        "    with open(os.path.join(self.exp_folder,'exp_resume.txt'), 'w') as f:\n",
        "      stri = \"\"\n",
        "      settings = [a for a in dir(experiment) if not a.startswith('__') and not callable(getattr(experiment, a))]\n",
        "      for setting in settings:\n",
        "        stri = stri + setting+\": \"+str(getattr(experiment, setting))+'\\n'\n",
        "      f.writelines(stri)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79mUnKTS2vil"
      },
      "source": [
        "INICIALIZACION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PZtHwd7OsN6"
      },
      "source": [
        "MODELOS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGDPLWIeOr4H"
      },
      "source": [
        "\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "\n",
        "def MobileNet_model(experiment,categories=64):\n",
        "  inp = Input(shape=(None, experiment.width, experiment.height, 3))\n",
        "  base_model = MobileNet(input_shape=(experiment.width, experiment.height, 3),\n",
        "                                  include_top=False,\n",
        "                                  weights='imagenet',\n",
        "                                  input_tensor=None,\n",
        "                                  pooling='avg',)\n",
        "  for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "  x = TimeDistributed(base_model)(inp)\n",
        "  for i in range(len(experiment.layers)-1):\n",
        "    x = GRU(units=experiment.layers[i]['units'], \n",
        "            return_sequences=True, \n",
        "            activation=experiment.activation, \n",
        "            kernel_initializer=experiment.initializer(), \n",
        "            kernel_regularizer=experiment.regularizer)(x)\n",
        "  x = GRU(units=experiment.layers[-1]['units'], \n",
        "          return_sequences=False, \n",
        "          activation=experiment.activation,\n",
        "          kernel_initializer=experiment.initializer(), \n",
        "          kernel_regularizer=experiment.regularizer)(x)  \n",
        "  x = Dense(experiment.dense_layer, activation=experiment.activation, kernel_initializer=experiment.initializer(), kernel_regularizer=experiment.regularizer)(x)\n",
        "  x = Dense(categories, activation='softmax')(x)\n",
        "  model = Model(inputs=inp, outputs=x,name=experiment.model_name)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XbpfKIzO3wk"
      },
      "source": [
        "GENERADOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40nXWUp-EdF2"
      },
      "source": [
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "def generator_cropped(videos_path,samples, batch_size, categories=64):\n",
        "  \"\"\"\n",
        "    samples serian los nombres de archivos\n",
        "  \"\"\"\n",
        "  height_upper_limit = int(1080/4)\n",
        "  width_upper_limit = int(1920/4)\n",
        "  height_to_crop = int(height_upper_limit*3)\n",
        "  width_to_crop = int(width_upper_limit*3)\n",
        "  w = experiment.width\n",
        "  h = experiment.height\n",
        "  num_samples = len(samples) \n",
        "  while True:\n",
        "    for offset in range(0,num_samples, batch_size):\n",
        "      batch_samples = samples[offset:offset+batch_size]\n",
        "      X_train = []\n",
        "      y_train = []\n",
        "      for batch_sample in batch_samples:\n",
        "        fname, w_limit, h_limit = batch_sample.split(\" \")\n",
        "        w_limit, h_limit = int(w_limit), int(h_limit)\n",
        "        vidcap = cv2.VideoCapture(videos_path+fname)      \n",
        "        #vidcap = imageio.get_reader(videos_path+batch_sample)\n",
        "        \n",
        "        frames = []\n",
        "        suc, frame = vidcap.read()                \n",
        "        #for i, frame in enumerate(vidcap):\n",
        "        while suc: \n",
        "          frame = frame[h_limit:h_limit+height_to_crop,w_limit:w_limit+width_to_crop,:]\n",
        "          frame = cv2.resize(frame, (w,h), interpolation=cv2.INTER_AREA)\n",
        "          frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if (experiment.channels==3) else np.expand_dims(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),2)\n",
        "          frame = frame.astype(np.float32)\n",
        "          frame /= 255.0          \n",
        "\n",
        "          frames.append(frame)\n",
        "          suc, frame = vidcap.read()  \n",
        "             \n",
        "        X_train.append(frames)        \n",
        "        cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "        y_train.append(to_categorical(cat, num_classes=categories))      \n",
        "      try:\n",
        "        X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "      except:\n",
        "        print(\"Trying again...\")\n",
        "        try:\n",
        "          X_train = np.asarray(X_train, dtype=np.float32)\n",
        "        except:\n",
        "          print(\"failed reading\")      \n",
        "      y_train = np.asarray(y_train)      \n",
        "      yield X_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw9g3NkoaqML"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import mobilenet\n",
        "def generator(videos_path,samples, batch_size, categories=64):\n",
        "  \"\"\"\n",
        "    samples serian los nombres de archivos\n",
        "  \"\"\"\n",
        "  w = experiment.width\n",
        "  h = experiment.height\n",
        "  num_samples = len(samples) \n",
        "  while True:\n",
        "    for offset in range(0,num_samples, batch_size):\n",
        "      batch_samples = samples[offset:offset+batch_size]      \n",
        "      X_train = []\n",
        "      y_train = []\n",
        "      for batch_sample in batch_samples:\n",
        "        vidcap = cv2.VideoCapture(os.path.join(videos_path,batch_sample))              \n",
        "        frames = []\n",
        "        suc, frame = vidcap.read()              \n",
        "        while suc: \n",
        "          frame = cv2.resize(frame, (w,h), interpolation=cv2.INTER_AREA)\n",
        "          frame = mobilenet.preprocess_input(frame)   \n",
        "          frames.append(frame)\n",
        "          suc, frame = vidcap.read()\n",
        "        \n",
        "        X_train.append(frames)        \n",
        "        cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "        y_train.append(to_categorical(cat, num_classes=categories))      \n",
        "      \n",
        "      try: \n",
        "        X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "      except:\n",
        "        print(\"frames len: \"+str(len(frames)))\n",
        "        try:\n",
        "          print(\"Intentando de vuelta\")\n",
        "          X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "        except:\n",
        "          print(len(X_train))\n",
        "        print(\"Long de video: \"+str(vidcap.get(7)))     \n",
        "      y_train = np.asarray(y_train)      \n",
        "      yield X_train, y_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISFbJSv_2zMb"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import mobilenet\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import math \n",
        "class SequenceGenerator(Sequence):\n",
        "  def __init__(self, videos_path, samples,batch_size, width, height, channels):\n",
        "    self.batch_size = batch_size\n",
        "    self.videos_path = videos_path\n",
        "    self.samples = samples\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.channels = channels\n",
        "    self.len = len(os.listdir(videos_path))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch_samples = self.samples[idx * self.batch_size:(idx+1)*self.batch_size]\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for batch_sample in batch_samples:      \n",
        "      vidcap = cv2.VideoCapture(os.path.join(self.videos_path,batch_sample))              \n",
        "      frames = []\n",
        "      suc, frame = vidcap.read()              \n",
        "      while suc: \n",
        "        frame = cv2.resize(frame, (self.width,self.height), interpolation=cv2.INTER_AREA)\n",
        "        frame = mobilenet.preprocess_input(frame)   \n",
        "        frames.append(frame)\n",
        "        suc, frame = vidcap.read()\n",
        "      \n",
        "      X_train.append(frames)        \n",
        "      cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "      y_train.append(to_categorical(cat, num_classes=64))      \n",
        "        \n",
        "    X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "    y_train = np.asarray(y_train)      \n",
        "    return X_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivoGjJTG7gd8"
      },
      "source": [
        "INICIALIZACION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_YgFppo7gBv"
      },
      "source": [
        "\n",
        "def changeTestName(test_names):\n",
        "  for i in range(len(test_names)):\n",
        "    test_names_cleaned = list(map(lambda sample: \"_\".join(sample.split('_')[:-1])+\".mp4\", test_names))\n",
        "  return list(set(test_names_cleaned))\n",
        "\n",
        "def changeTestNameSimplesampling(test_names):\n",
        "  for i in range(len(test_names)):\n",
        "    test_names_cleaned = list(map(lambda sample: sample.split('.')[0]+\".mp4\", test_names))\n",
        "  return list(set(test_names_cleaned))\n",
        "\"\"\"\n",
        "print(os.listdir(\"/content/gdrive/My Drive/Inform√°tica/Tesina/Experimentos/MobileNet+Conv\"))\n",
        "print(\"Ingrese el nombre del experimento: \")\n",
        "exp_name = input()\n",
        "if not os.path.exists(os.path.join(\"/content/gdrive/My Drive/Inform√°tica/Tesina/Experimentos/MobileNet+Conv\", exp_name)):\n",
        "  os.mkdir(os.path.join(\"/content/gdrive/My Drive/Inform√°tica/Tesina/Experimentos/MobileNet+Conv\", exp_name))\n",
        "print(os.listdir(os.path.join(\"/content/gdrive/My Drive/Inform√°tica/Tesina/Experimentos/MobileNet+Conv\", exp_name)))\n",
        "print(\"Nombre de modelo [Baseline]: \")\n",
        "model_name = input()\n",
        "model_name = \"baseline\" if model_name == '' else model_name\n",
        "\"\"\"\n",
        "experiment = Experiment(DATASET, MODEL_NAME, FOLD_NAME)\n",
        "experiment.save_resume()\n",
        "weights_path = experiment.weights_path\n",
        "pickles_path = experiment.pickles_path\n",
        "videos_path = experiment.videos_path\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIaJWpOR6XGu"
      },
      "source": [
        "\n",
        "TRAIN VAL TEST SPLIT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcw2E8Om7o0O"
      },
      "source": [
        "\n",
        "with open(os.path.join(experiment.names_path,f'train_{experiment.fold_name}_{experiment.files_posfix}.txt'), 'r') as f:\n",
        "  train = f.read().split('\\n')\n",
        "with open(os.path.join(experiment.names_path,f'val_{experiment.fold_name}_{experiment.files_posfix}.txt'), 'r') as f:\n",
        "  val = f.read().split('\\n')\n",
        "with open(os.path.join(experiment.names_path,f'test_{experiment.fold_name}_{experiment.files_posfix}.txt'), 'r') as f:\n",
        "  test = f.read().split('\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRwXfuIO9xF3"
      },
      "source": [
        "def get_model_memory_usage(batch_size, model):\n",
        "    import numpy as np\n",
        "    try:\n",
        "        from keras import backend as K\n",
        "    except:\n",
        "        from tensorflow.keras import backend as K\n",
        "\n",
        "    shapes_mem_count = 0\n",
        "    internal_model_mem_count = 0\n",
        "    for l in model.layers:\n",
        "        layer_type = l.__class__.__name__\n",
        "        if layer_type == 'Model':\n",
        "            internal_model_mem_count += get_model_memory_usage(batch_size, l)\n",
        "        single_layer_mem = 1\n",
        "        out_shape = l.output_shape\n",
        "        if type(out_shape) is list:\n",
        "            out_shape = out_shape[0]\n",
        "        for s in out_shape:\n",
        "            if s is None:\n",
        "                continue\n",
        "            single_layer_mem *= s\n",
        "        shapes_mem_count += single_layer_mem\n",
        "\n",
        "    trainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])\n",
        "    non_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n",
        "\n",
        "    number_size = 4.0\n",
        "    if K.floatx() == 'float16':\n",
        "        number_size = 2.0\n",
        "    if K.floatx() == 'float64':\n",
        "        number_size = 8.0\n",
        "\n",
        "    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)\n",
        "    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count\n",
        "    return gbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "EpbKk7s2-E9r",
        "outputId": "21ed848d-a794-4ecc-beb6-4f9ed69d7f07"
      },
      "source": [
        "print(get_model_memory_usage(32, model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a3e95d59e705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_model_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA_nNyB793iW",
        "outputId": "5ff9bc20-f6db-462b-f361-5fc55312270d"
      },
      "source": [
        "print(get_model_memory_usage(32, model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg5Z2Kjl7p64"
      },
      "source": [
        "ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6JoWg3rGIhU",
        "outputId": "e4b7fe70-4e9d-49bc-f262-cceb40041c92"
      },
      "source": [
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "batch_size = experiment.batch_size\n",
        "\n",
        "\n",
        "# NO GUARDO MODELO POR AHORA\n",
        "\n",
        "if f\"{experiment.model_name}.pickle\" in os.listdir(experiment.exp_folder):  \n",
        "  with open(os.path.join(experiment.exp_folder, f\"{experiment.model_name}.pickle\"), 'rb') as pklfile:\n",
        "    conf = pickle.load(pklfile)    \n",
        "    model = Model.from_config(conf)\n",
        "else:  \n",
        "  model = MobileNet_model(experiment)\n",
        "  #with open(os.path.join(experiment.exp_folder, f\"{experiment.model_name}.pickle\"), 'wb') as pklfile:\n",
        "  #  conf = model.get_config()\n",
        "  #  pickle.dump(conf, pklfile)\n",
        "model.summary()\n",
        "\n",
        "with open(os.path.join(experiment.exp_folder, f'{experiment.model_name}.txt'), 'w') as f:\n",
        "  model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "if '{}_epoch_{}.h5'.format(experiment.model_name, experiment.current_epoch) in os.listdir(weights_path):\n",
        "  model.load_weights(os.path.join(experiment.weights_path,'{}_epoch_{}.h5'.format(experiment.model_name, experiment.current_epoch)))\n",
        "\n",
        "\n",
        "adam = Adam(learning_rate=experiment.lr)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if experiment.testing:\n",
        "  print(\"Estas seguro que queres testear? [y/n]\")\n",
        "  if (input() == 'y'):    \n",
        "    eval_dict = model.evaluate(\n",
        "        generator(videos_path, test, 1)\n",
        "      , steps=int(len(test)/1)\n",
        "      , batch_size=1\n",
        "      , verbose = 1\n",
        "      , return_dict=True\n",
        "    )\n",
        "    with open(os.path.join(experiment.exp_folder, f'evaluation_epoch{experiment.current_epoch}_toLength.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(eval_dict, pkl)\n",
        "  else: \n",
        "    print(\"Cambia el experimento entonces en experiment.testing\")\n",
        "else:\n",
        "  train_gen = SequenceGenerator(videos_path, train, batch_size, experiment.width, experiment.height, experiment.channels)    \n",
        "  val_gen = SequenceGenerator(videos_path, val, batch_size, experiment.width, experiment.height, experiment.channels)    \n",
        "  history = model.fit(\n",
        "    train_gen\n",
        "    , steps_per_epoch=int(len(train)/batch_size)\n",
        "    , validation_data = val_gen\n",
        "    ,validation_steps = int(len(val)/batch_size)\n",
        "    , epochs = experiment.epochs_to_train\n",
        "    , verbose = 1\n",
        "    , shuffle = False    \n",
        "    , callbacks=list(experiment.callbacks)\n",
        "    , use_multiprocessing=True\n",
        "      ,workers=4\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"rgb128_2fold\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, 128, 128, 3 0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 1024)        3228864   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, None, 128)         443136    \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 128)               99072     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 3,795,840\n",
            "Trainable params: 566,976\n",
            "Non-trainable params: 3,228,864\n",
            "_________________________________________________________________\n",
            "Epoch 1/48\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - ETA: 0s - loss: 1.9945 - accuracy: 0.4965 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - 4074s 13s/step - loss: 1.9910 - accuracy: 0.4974 - val_loss: 0.1038 - val_accuracy: 0.9719\n",
            "Epoch 2/48\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9699 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - 3887s 13s/step - loss: 0.0987 - accuracy: 0.9699 - val_loss: 0.0098 - val_accuracy: 0.9984\n",
            "Epoch 3/48\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9859 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - 3897s 13s/step - loss: 0.0431 - accuracy: 0.9859 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
            "Epoch 4/48\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9937 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "300/300 [==============================] - 3985s 13s/step - loss: 0.0210 - accuracy: 0.9936 - val_loss: 0.0762 - val_accuracy: 0.9703\n",
            "Epoch 5/48\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            " 53/300 [====>.........................] - ETA: 50:21 - loss: 0.1143 - accuracy: 0.9607"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZPHnIZY-dAi"
      },
      "source": [
        "CROSS LENGTH TESTING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJLrAnaedcfU"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications import mobilenet\n",
        "from keras.utils import Sequence\n",
        "import math \n",
        "class SequenceGenerator(Sequence):\n",
        "  def __init__(self, videos_path, samples,batch_size, width, height, channels):\n",
        "    self.batch_size = batch_size\n",
        "    self.videos_path = videos_path\n",
        "    self.samples = samples\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.channels = channels\n",
        "    self.len = len(os.listdir(videos_path))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch_samples = self.samples[idx * self.batch_size:(idx+1)*self.batch_size]\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for batch_sample in batch_samples:      \n",
        "      vidcap = cv2.VideoCapture(os.path.join(self.videos_path,batch_sample))              \n",
        "      frames = []\n",
        "      suc, frame = vidcap.read()              \n",
        "      while suc: \n",
        "        frame = cv2.resize(frame, (self.width,self.height), interpolation=cv2.INTER_AREA)\n",
        "        frame = mobilenet.preprocess_input(frame)   \n",
        "        frames.append(frame)\n",
        "        suc, frame = vidcap.read()\n",
        "      \n",
        "      X_train.append(frames)        \n",
        "      cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "      y_train.append(to_categorical(cat, num_classes=64))      \n",
        "        \n",
        "    X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "    y_train = np.asarray(y_train)      \n",
        "    return X_train, y_train\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK7N373Crv1D"
      },
      "source": [
        "DATASET = 'SimpleSampling32'\n",
        "import numpy as np\n",
        "import pickle\n",
        "import functools as ft\n",
        "from keras import Model\n",
        "from keras.optimizers import Adam\n",
        "def addHistories(h1, h2):    \n",
        "    for key in [\"accuracy\", \"loss\", \"val_loss\", \"val_accuracy\"]:\n",
        "        h2[key].append(h1[key]) #ya no es una lista asi que puedo acceder a la metrica directo \n",
        "    return h2\n",
        "\n",
        "basePath = \"/content/gdrive/My Drive/Inform√°tica/Tesina/\"\n",
        "baseFolder = basePath+'Experimentos/MobileNet+Conv/'+DATASET\n",
        "namesPath = basePath+'DatasetsNames/'+DATASET\n",
        "\n",
        "best_epochs = {}\n",
        "mean_acc = {'8':[],'16':[], '32':[] }\n",
        "videos_path = {'8':basePath+'Videos/Multisampling8/','16':basePath+'Videos/Multisampling16/', '32':basePath+'Videos/SimpleSampling32/' }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdKgjQ7GrGdL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24d37fe-d02b-4909-c16f-bc951ba4125c"
      },
      "source": [
        "\n",
        "for fold in ['2fold','3fold','4fold']:\n",
        "  modelName = 'rgb'+fold\n",
        "  expFolder = os.path.join(baseFolder,modelName)\n",
        "  if f\"{modelName}.pickle\" in os.listdir(expFolder):  \n",
        "    with open(os.path.join(expFolder, f\"{modelName}.pickle\"), 'rb') as pklfile:\n",
        "      conf = pickle.load(pklfile)    \n",
        "      model = Model.from_config(conf)\n",
        "  else:\n",
        "    print(\"no encuentro modelo\")\n",
        "  adam = Adam(learning_rate=0.001)\n",
        "  model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  EXP_PKL_PATH = os.path.join(expFolder, 'Pickles')  \n",
        "  pkl_files = os.listdir(EXP_PKL_PATH)\n",
        "  fls_s = sorted(pkl_files, key=lambda x: int(x.split('_')[-1].split('.')[0])) #Exp_18_epoch_2.pickle\n",
        "\n",
        "  pkls = []\n",
        "  for pkl_file in fls_s:\n",
        "      with open(os.path.join(EXP_PKL_PATH ,pkl_file), 'rb') as pkl:\n",
        "          h_temp = pickle.load(pkl)        \n",
        "          print(h_temp)\n",
        "          if 'val_loss' in h_temp.keys():\n",
        "            pkls.append(h_temp)\n",
        "\n",
        "\n",
        "  init_dict = {}\n",
        "  for k in [\"accuracy\", \"loss\", \"val_loss\", \"val_accuracy\"]:\n",
        "    init_dict[k] = []\n",
        "  hs = ft.reduce(lambda a,b: addHistories(b,a), pkls, init_dict)\n",
        "  best_epoch = np.array(hs['val_accuracy']).argmax()+1\n",
        "  best_epochs[fold] = (best_epoch,hs['val_accuracy'][best_epoch-1])\n",
        "  model.load_weights(os.path.join(expFolder,'Weights','{}_epoch_{}.h5'.format(modelName, best_epoch)))\n",
        "  print(hs['val_accuracy'])\n",
        "  print(best_epoch)\n",
        "  with open(os.path.join(namesPath,f'test_{fold}_{DATASET}.txt'), 'r') as f:\n",
        "    test = f.read().split('\\n')  \n",
        "  \n",
        "  for num_frames in ['32']:\n",
        "    path = videos_path[num_frames]\n",
        "    gen = SequenceGenerator(path, test, 32, 64, 64, 3)    \n",
        "    eval_dict = model.evaluate(\n",
        "          gen\n",
        "        , steps=int(len(test)/32)\n",
        "        , batch_size=32\n",
        "        , verbose = 1\n",
        "        , return_dict=True\n",
        "      )\n",
        "    mean_acc[num_frames].append(eval_dict['accuracy'])\n",
        "    with open(os.path.join(expFolder, f'evaluation_epoch{best_epoch}_{num_frames}frames.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(eval_dict, pkl)    \n",
        "  \n",
        "with open(os.path.join(baseFolder, f'tests.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(mean_acc, pkl)\n",
        "\n",
        "with open(os.path.join(baseFolder, f'bestEpochs.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(best_epochs, pkl)        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "{'loss': 3.027933120727539, 'accuracy': 0.20928029716014862, 'val_loss': 1.8524188995361328, 'val_accuracy': 0.4285714328289032}\n",
            "{'loss': 1.2071391344070435, 'accuracy': 0.6169507503509521, 'val_loss': 0.9018262028694153, 'val_accuracy': 0.7232142686843872}\n",
            "{'loss': 0.5611450672149658, 'accuracy': 0.8271780014038086, 'val_loss': 0.6210358738899231, 'val_accuracy': 0.796875}\n",
            "{'loss': 0.30858346819877625, 'accuracy': 0.9100378751754761, 'val_loss': 0.43679705262184143, 'val_accuracy': 0.8616071343421936}\n",
            "{'loss': 0.21335728466510773, 'accuracy': 0.9346590638160706, 'val_loss': 0.421249121427536, 'val_accuracy': 0.8526785969734192}\n",
            "{'loss': 0.15428322553634644, 'accuracy': 0.9488636255264282, 'val_loss': 0.3881329596042633, 'val_accuracy': 0.875}\n",
            "{'loss': 0.09316202998161316, 'accuracy': 0.9753788113594055, 'val_loss': 0.3241279125213623, 'val_accuracy': 0.9040178656578064}\n",
            "{'loss': 0.061451997607946396, 'accuracy': 0.9796401262283325, 'val_loss': 0.3362032473087311, 'val_accuracy': 0.9129464030265808}\n",
            "{'loss': 0.04949337989091873, 'accuracy': 0.986268937587738, 'val_loss': 0.31790751218795776, 'val_accuracy': 0.9017857313156128}\n",
            "{'loss': 0.050284095108509064, 'accuracy': 0.9848484992980957, 'val_loss': 0.4853820204734802, 'val_accuracy': 0.8683035969734192}\n",
            "{'loss': 0.09951560944318771, 'accuracy': 0.9696969985961914, 'val_loss': 0.24617527425289154, 'val_accuracy': 0.9285714030265808}\n",
            "{'loss': 0.07716764509677887, 'accuracy': 0.9753788113594055, 'val_loss': 0.3833809792995453, 'val_accuracy': 0.8839285969734192}\n",
            "{'loss': 0.09919628500938416, 'accuracy': 0.9692234992980957, 'val_loss': 0.3453526794910431, 'val_accuracy': 0.8995535969734192}\n",
            "{'loss': 0.06801808625459671, 'accuracy': 0.9786931872367859, 'val_loss': 0.2885661721229553, 'val_accuracy': 0.9174107313156128}\n",
            "{'loss': 0.029112515971064568, 'accuracy': 0.9933711886405945, 'val_loss': 0.24416902661323547, 'val_accuracy': 0.9285714030265808}\n",
            "{'loss': 0.0692630186676979, 'accuracy': 0.9805871248245239, 'val_loss': 0.3938538730144501, 'val_accuracy': 0.890625}\n",
            "{'loss': 0.05685469135642052, 'accuracy': 0.9834280014038086, 'val_loss': 0.3233223855495453, 'val_accuracy': 0.9129464030265808}\n",
            "{'loss': 0.060592927038669586, 'accuracy': 0.9829545617103577, 'val_loss': 0.3717615306377411, 'val_accuracy': 0.9151785969734192}\n",
            "{'loss': 0.04507334902882576, 'accuracy': 0.9867424368858337, 'val_loss': 0.2853124737739563, 'val_accuracy': 0.9174107313156128}\n",
            "{'loss': 0.05200226604938507, 'accuracy': 0.9815340638160706, 'val_loss': 0.3549194931983948, 'val_accuracy': 0.90625}\n",
            "{'loss': 0.08189408481121063, 'accuracy': 0.9767992496490479, 'val_loss': 0.36829161643981934, 'val_accuracy': 0.9129464030265808}\n",
            "{'loss': 0.05553621053695679, 'accuracy': 0.9820075631141663, 'val_loss': 0.3471798896789551, 'val_accuracy': 0.9174107313156128}\n",
            "{'loss': 0.06704432517290115, 'accuracy': 0.9786931872367859, 'val_loss': 0.2778930068016052, 'val_accuracy': 0.9308035969734192}\n",
            "{'loss': 0.032905638217926025, 'accuracy': 0.9924242496490479, 'val_loss': 0.22819459438323975, 'val_accuracy': 0.9419642686843872}\n",
            "{'loss': 0.012798416428267956, 'accuracy': 0.9966856241226196, 'val_loss': 0.20564498007297516, 'val_accuracy': 0.9441964030265808}\n",
            "{'loss': 0.014694544486701488, 'accuracy': 0.9957386255264282, 'val_loss': 0.1690611094236374, 'val_accuracy': 0.9553571343421936}\n",
            "{'loss': 0.0013285240856930614, 'accuracy': 1.0, 'val_loss': 0.14593689143657684, 'val_accuracy': 0.953125}\n",
            "{'loss': 0.04364928603172302, 'accuracy': 0.9881628751754761, 'val_loss': 0.46022653579711914, 'val_accuracy': 0.8995535969734192}\n",
            "{'loss': 0.05966890603303909, 'accuracy': 0.9839015007019043, 'val_loss': 0.574110209941864, 'val_accuracy': 0.8995535969734192}\n",
            "{'loss': 0.04048677533864975, 'accuracy': 0.986268937587738, 'val_loss': 0.37493300437927246, 'val_accuracy': 0.9017857313156128}\n",
            "{'loss': 0.0413694754242897, 'accuracy': 0.986268937587738, 'val_loss': 0.26403817534446716, 'val_accuracy': 0.9419642686843872}\n",
            "{'loss': 0.0780048668384552, 'accuracy': 0.9777461886405945, 'val_loss': 0.5047193169593811, 'val_accuracy': 0.8772321343421936}\n",
            "{'loss': 0.08944300562143326, 'accuracy': 0.9739583134651184, 'val_loss': 0.4086568355560303, 'val_accuracy': 0.9040178656578064}\n",
            "{'loss': 0.017664723098278046, 'accuracy': 0.9943181872367859, 'val_loss': 0.20173940062522888, 'val_accuracy': 0.9508928656578064}\n",
            "{'loss': 0.003612969070672989, 'accuracy': 0.9995265007019043, 'val_loss': 0.1817684918642044, 'val_accuracy': 0.9553571343421936}\n",
            "{'loss': 0.0007606383878737688, 'accuracy': 1.0, 'val_loss': 0.17334343492984772, 'val_accuracy': 0.953125}\n",
            "{'loss': 0.0002546934993006289, 'accuracy': 1.0, 'val_loss': 0.16976098716259003, 'val_accuracy': 0.9508928656578064}\n",
            "{'loss': 0.0002015157660935074, 'accuracy': 1.0, 'val_loss': 0.1688138246536255, 'val_accuracy': 0.9508928656578064}\n",
            "{'loss': 0.0001712725788820535, 'accuracy': 1.0, 'val_loss': 0.16806480288505554, 'val_accuracy': 0.9508928656578064}\n",
            "[0.4285714328289032, 0.7232142686843872, 0.796875, 0.8616071343421936, 0.8526785969734192, 0.875, 0.9040178656578064, 0.9129464030265808, 0.9017857313156128, 0.8683035969734192, 0.9285714030265808, 0.8839285969734192, 0.8995535969734192, 0.9174107313156128, 0.9285714030265808, 0.890625, 0.9129464030265808, 0.9151785969734192, 0.9174107313156128, 0.90625, 0.9129464030265808, 0.9174107313156128, 0.9308035969734192, 0.9419642686843872, 0.9441964030265808, 0.9553571343421936, 0.953125, 0.8995535969734192, 0.8995535969734192, 0.9017857313156128, 0.9419642686843872, 0.8772321343421936, 0.9040178656578064, 0.9508928656578064, 0.9553571343421936, 0.953125, 0.9508928656578064, 0.9508928656578064, 0.9508928656578064]\n",
            "26\n",
            "20/20 [==============================] - 477s 24s/step - loss: 0.1677 - accuracy: 0.9543\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "{'loss': 2.9846389293670654, 'accuracy': 0.2296401560306549, 'val_loss': 1.8093273639678955, 'val_accuracy': 0.453125}\n",
            "{'loss': 1.1805248260498047, 'accuracy': 0.6358901262283325, 'val_loss': 0.8874538540840149, 'val_accuracy': 0.7165178656578064}\n",
            "{'loss': 0.5532743334770203, 'accuracy': 0.8271780014038086, 'val_loss': 0.6282350420951843, 'val_accuracy': 0.8013392686843872}\n",
            "{'loss': 0.30984169244766235, 'accuracy': 0.90625, 'val_loss': 0.4722890853881836, 'val_accuracy': 0.8616071343421936}\n",
            "{'loss': 0.21786366403102875, 'accuracy': 0.9294507503509521, 'val_loss': 0.3606265187263489, 'val_accuracy': 0.9017857313156128}\n",
            "{'loss': 0.11102201789617538, 'accuracy': 0.9673295617103577, 'val_loss': 0.32788801193237305, 'val_accuracy': 0.9241071343421936}\n",
            "{'loss': 0.06186635419726372, 'accuracy': 0.9834280014038086, 'val_loss': 0.2545790672302246, 'val_accuracy': 0.9308035969734192}\n",
            "{'loss': 0.04838276654481888, 'accuracy': 0.9910038113594055, 'val_loss': 0.24516665935516357, 'val_accuracy': 0.9419642686843872}\n",
            "{'loss': 0.058606572449207306, 'accuracy': 0.9820075631141663, 'val_loss': 0.32247546315193176, 'val_accuracy': 0.921875}\n",
            "{'loss': 0.06670280545949936, 'accuracy': 0.9801136255264282, 'val_loss': 0.2969132363796234, 'val_accuracy': 0.921875}\n",
            "{'loss': 0.12022724747657776, 'accuracy': 0.9583333134651184, 'val_loss': 0.4882540702819824, 'val_accuracy': 0.8794642686843872}\n",
            "{'loss': 0.05787741392850876, 'accuracy': 0.9810606241226196, 'val_loss': 0.417667418718338, 'val_accuracy': 0.8772321343421936}\n",
            "{'loss': 0.055379483848810196, 'accuracy': 0.9848484992980957, 'val_loss': 0.3908229470252991, 'val_accuracy': 0.8973214030265808}\n",
            "{'loss': 0.06453326344490051, 'accuracy': 0.9801136255264282, 'val_loss': 0.4129047989845276, 'val_accuracy': 0.8660714030265808}\n",
            "{'loss': 0.041195742785930634, 'accuracy': 0.9900568127632141, 'val_loss': 0.3942126929759979, 'val_accuracy': 0.90625}\n",
            "{'loss': 0.06749211251735687, 'accuracy': 0.9772727489471436, 'val_loss': 0.45421102643013, 'val_accuracy': 0.8950892686843872}\n",
            "{'loss': 0.07836654782295227, 'accuracy': 0.9758522510528564, 'val_loss': 0.287971168756485, 'val_accuracy': 0.9196428656578064}\n",
            "{'loss': 0.059660594910383224, 'accuracy': 0.9810606241226196, 'val_loss': 0.2653439939022064, 'val_accuracy': 0.9397321343421936}\n",
            "{'loss': 0.025476602837443352, 'accuracy': 0.9933711886405945, 'val_loss': 0.22841687500476837, 'val_accuracy': 0.9486607313156128}\n",
            "{'loss': 0.03589409589767456, 'accuracy': 0.9900568127632141, 'val_loss': 0.34987762570381165, 'val_accuracy': 0.921875}\n",
            "{'loss': 0.032647907733917236, 'accuracy': 0.9900568127632141, 'val_loss': 0.27643755078315735, 'val_accuracy': 0.9352678656578064}\n",
            "{'loss': 0.09608400613069534, 'accuracy': 0.9711174368858337, 'val_loss': 0.3968684673309326, 'val_accuracy': 0.8973214030265808}\n",
            "{'loss': 0.08572908490896225, 'accuracy': 0.9749053120613098, 'val_loss': 0.3806867003440857, 'val_accuracy': 0.8995535969734192}\n",
            "[0.453125, 0.7165178656578064, 0.8013392686843872, 0.8616071343421936, 0.9017857313156128, 0.9241071343421936, 0.9308035969734192, 0.9419642686843872, 0.921875, 0.921875, 0.8794642686843872, 0.8772321343421936, 0.8973214030265808, 0.8660714030265808, 0.90625, 0.8950892686843872, 0.9196428656578064, 0.9397321343421936, 0.9486607313156128, 0.921875, 0.9352678656578064, 0.8973214030265808, 0.8995535969734192]\n",
            "19\n",
            "20/20 [==============================] - 488s 25s/step - loss: 0.2754 - accuracy: 0.9351\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "{'loss': 2.9081997871398926, 'accuracy': 0.24857954680919647, 'val_loss': 1.7360414266586304, 'val_accuracy': 0.4620535671710968}\n",
            "{'loss': 1.120718002319336, 'accuracy': 0.6676136255264282, 'val_loss': 0.9387186765670776, 'val_accuracy': 0.6919642686843872}\n",
            "{'loss': 0.4947766363620758, 'accuracy': 0.8541666865348816, 'val_loss': 0.5128218531608582, 'val_accuracy': 0.8214285969734192}\n",
            "{'loss': 0.30021899938583374, 'accuracy': 0.9034090638160706, 'val_loss': 0.44620397686958313, 'val_accuracy': 0.8504464030265808}\n",
            "{'loss': 0.21730242669582367, 'accuracy': 0.9303977489471436, 'val_loss': 0.49240031838417053, 'val_accuracy': 0.8638392686843872}\n",
            "{'loss': 0.13410316407680511, 'accuracy': 0.9630681872367859, 'val_loss': 0.35356858372688293, 'val_accuracy': 0.9040178656578064}\n",
            "{'loss': 0.09221258014440536, 'accuracy': 0.9753788113594055, 'val_loss': 0.33169540762901306, 'val_accuracy': 0.8839285969734192}\n",
            "{'loss': 0.07302644848823547, 'accuracy': 0.9791666865348816, 'val_loss': 0.24031171202659607, 'val_accuracy': 0.9263392686843872}\n",
            "{'loss': 0.040977876633405685, 'accuracy': 0.9914772510528564, 'val_loss': 0.21551357209682465, 'val_accuracy': 0.9263392686843872}\n",
            "{'loss': 0.04169532284140587, 'accuracy': 0.9900568127632141, 'val_loss': 0.28110164403915405, 'val_accuracy': 0.9174107313156128}\n",
            "{'loss': 0.07702308893203735, 'accuracy': 0.9753788113594055, 'val_loss': 0.3391851782798767, 'val_accuracy': 0.9017857313156128}\n",
            "{'loss': 0.07229156792163849, 'accuracy': 0.9791666865348816, 'val_loss': 0.26493141055107117, 'val_accuracy': 0.9330357313156128}\n",
            "{'loss': 0.11721819639205933, 'accuracy': 0.9635416865348816, 'val_loss': 0.4967058002948761, 'val_accuracy': 0.8660714030265808}\n",
            "{'loss': 0.10593955218791962, 'accuracy': 0.9663825631141663, 'val_loss': 0.2985951006412506, 'val_accuracy': 0.9107142686843872}\n",
            "{'loss': 0.06531274318695068, 'accuracy': 0.9772727489471436, 'val_loss': 0.22989153861999512, 'val_accuracy': 0.9375}\n",
            "{'loss': 0.04432113096117973, 'accuracy': 0.9876893758773804, 'val_loss': 0.22908934950828552, 'val_accuracy': 0.9419642686843872}\n",
            "{'loss': 0.009787875227630138, 'accuracy': 0.9976325631141663, 'val_loss': 0.2080695927143097, 'val_accuracy': 0.9464285969734192}\n",
            "{'loss': 0.0019153738394379616, 'accuracy': 1.0, 'val_loss': 0.16721263527870178, 'val_accuracy': 0.9486607313156128}\n",
            "{'loss': 0.0004705983155872673, 'accuracy': 1.0, 'val_loss': 0.15580616891384125, 'val_accuracy': 0.9598214030265808}\n",
            "{'loss': 0.00030828602029941976, 'accuracy': 1.0, 'val_loss': 0.15252898633480072, 'val_accuracy': 0.9620535969734192}\n",
            "{'loss': 0.00025402114260941744, 'accuracy': 1.0, 'val_loss': 0.15070591866970062, 'val_accuracy': 0.9620535969734192}\n",
            "{'loss': 0.0002165175974369049, 'accuracy': 1.0, 'val_loss': 0.14945824444293976, 'val_accuracy': 0.9642857313156128}\n",
            "{'loss': 0.00018824278959073126, 'accuracy': 1.0, 'val_loss': 0.14855727553367615, 'val_accuracy': 0.9642857313156128}\n",
            "{'loss': 0.000165911580552347, 'accuracy': 1.0, 'val_loss': 0.14789657294750214, 'val_accuracy': 0.9665178656578064}\n",
            "{'loss': 0.00014773602015338838, 'accuracy': 1.0, 'val_loss': 0.14741292595863342, 'val_accuracy': 0.9665178656578064}\n",
            "{'loss': 0.0001326289784628898, 'accuracy': 1.0, 'val_loss': 0.14706794917583466, 'val_accuracy': 0.9665178656578064}\n",
            "{'loss': 0.00011983673175564036, 'accuracy': 1.0, 'val_loss': 0.14683322608470917, 'val_accuracy': 0.9665178656578064}\n",
            "{'loss': 0.000108846346847713, 'accuracy': 1.0, 'val_loss': 0.1466883271932602, 'val_accuracy': 0.9665178656578064}\n",
            "{'loss': 0.05657853186130524, 'accuracy': 0.9839015007019043, 'val_loss': 0.2870446741580963, 'val_accuracy': 0.9241071343421936}\n",
            "{'loss': 0.0965990200638771, 'accuracy': 0.9696969985961914, 'val_loss': 0.30605608224868774, 'val_accuracy': 0.9285714030265808}\n",
            "{'loss': 0.02923385240137577, 'accuracy': 0.9928977489471436, 'val_loss': 0.2552594244480133, 'val_accuracy': 0.9330357313156128}\n",
            "{'loss': 0.02341744303703308, 'accuracy': 0.9938446879386902, 'val_loss': 0.22091878950595856, 'val_accuracy': 0.9397321343421936}\n",
            "{'loss': 0.04194040596485138, 'accuracy': 0.9886363744735718, 'val_loss': 0.19516770541667938, 'val_accuracy': 0.953125}\n",
            "{'loss': 0.04218783229589462, 'accuracy': 0.9876893758773804, 'val_loss': 0.37737414240837097, 'val_accuracy': 0.9107142686843872}\n",
            "{'loss': 0.09272400289773941, 'accuracy': 0.9711174368858337, 'val_loss': 0.41456660628318787, 'val_accuracy': 0.9017857313156128}\n",
            "{'loss': 0.09927749633789062, 'accuracy': 0.9659090638160706, 'val_loss': 0.22713987529277802, 'val_accuracy': 0.9441964030265808}\n",
            "{'loss': 0.011969334445893764, 'accuracy': 0.9976325631141663, 'val_loss': 0.17196562886238098, 'val_accuracy': 0.9575892686843872}\n",
            "{'loss': 0.015372718684375286, 'accuracy': 0.9971590638160706, 'val_loss': 0.156608447432518, 'val_accuracy': 0.9642857313156128}\n",
            "{'loss': 0.0029479411896318197, 'accuracy': 0.9990530014038086, 'val_loss': 0.13321606814861298, 'val_accuracy': 0.9665178656578064}\n",
            "{'loss': 0.0005118554690852761, 'accuracy': 1.0, 'val_loss': 0.1270868480205536, 'val_accuracy': 0.9709821343421936}\n",
            "{'loss': 0.0002568610943853855, 'accuracy': 1.0, 'val_loss': 0.12253595888614655, 'val_accuracy': 0.9709821343421936}\n",
            "{'loss': 0.0002062943094642833, 'accuracy': 1.0, 'val_loss': 0.11940836161375046, 'val_accuracy': 0.9732142686843872}\n",
            "{'loss': 0.00017470518650952727, 'accuracy': 1.0, 'val_loss': 0.11724209040403366, 'val_accuracy': 0.9754464030265808}\n",
            "{'loss': 0.0001520072837593034, 'accuracy': 1.0, 'val_loss': 0.11559152603149414, 'val_accuracy': 0.9754464030265808}\n",
            "{'loss': 0.00013454334111884236, 'accuracy': 1.0, 'val_loss': 0.1142619252204895, 'val_accuracy': 0.9754464030265808}\n",
            "{'loss': 0.00012054140825057402, 'accuracy': 1.0, 'val_loss': 0.11315196752548218, 'val_accuracy': 0.9754464030265808}\n",
            "{'loss': 0.00010898081382038072, 'accuracy': 1.0, 'val_loss': 0.11220178753137589, 'val_accuracy': 0.9754464030265808}\n",
            "[0.4620535671710968, 0.6919642686843872, 0.8214285969734192, 0.8504464030265808, 0.8638392686843872, 0.9040178656578064, 0.8839285969734192, 0.9263392686843872, 0.9263392686843872, 0.9174107313156128, 0.9017857313156128, 0.9330357313156128, 0.8660714030265808, 0.9107142686843872, 0.9375, 0.9419642686843872, 0.9464285969734192, 0.9486607313156128, 0.9598214030265808, 0.9620535969734192, 0.9620535969734192, 0.9642857313156128, 0.9642857313156128, 0.9665178656578064, 0.9665178656578064, 0.9665178656578064, 0.9665178656578064, 0.9665178656578064, 0.9241071343421936, 0.9285714030265808, 0.9330357313156128, 0.9397321343421936, 0.953125, 0.9107142686843872, 0.9017857313156128, 0.9441964030265808, 0.9575892686843872, 0.9642857313156128, 0.9665178656578064, 0.9709821343421936, 0.9709821343421936, 0.9732142686843872, 0.9754464030265808, 0.9754464030265808, 0.9754464030265808, 0.9754464030265808, 0.9754464030265808]\n",
            "43\n",
            "20/20 [==============================] - 482s 25s/step - loss: 0.2945 - accuracy: 0.9476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H-_s5CiRt7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b1e10a-3c85-4c41-8b4b-42f43c2664f0"
      },
      "source": [
        "os.listdir(EXP_PKL_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rgb2fold_epoch_1.pickle',\n",
              " 'rgb2fold_epoch_2.pickle',\n",
              " 'rgb2fold_epoch_3.pickle',\n",
              " 'rgb2fold_epoch_4.pickle',\n",
              " 'rgb2fold_epoch_5.pickle',\n",
              " 'rgb2fold_epoch_6.pickle',\n",
              " 'rgb2fold_epoch_7.pickle',\n",
              " 'rgb2fold_epoch_8.pickle',\n",
              " 'rgb2fold_epoch_9.pickle',\n",
              " 'rgb2fold_epoch_10.pickle',\n",
              " 'rgb2fold_epoch_11.pickle',\n",
              " 'rgb2fold_epoch_12.pickle',\n",
              " 'rgb2fold_epoch_13.pickle',\n",
              " 'rgb2fold_epoch_14.pickle',\n",
              " 'rgb2fold_epoch_15.pickle',\n",
              " 'rgb2fold_epoch_16.pickle',\n",
              " 'rgb2fold_epoch_17.pickle',\n",
              " 'rgb2fold_epoch_18.pickle',\n",
              " 'rgb2fold_epoch_19.pickle',\n",
              " 'rgb2fold_epoch_20.pickle',\n",
              " 'rgb2fold_epoch_21.pickle',\n",
              " 'rgb2fold_epoch_22.pickle',\n",
              " 'rgb2fold_epoch_23.pickle',\n",
              " 'rgb2fold_epoch_24.pickle',\n",
              " 'rgb2fold_epoch_25.pickle',\n",
              " 'rgb2fold_epoch_26.pickle',\n",
              " 'rgb2fold_epoch_27.pickle',\n",
              " 'rgb2fold_epoch_28 (1).pickle',\n",
              " 'rgb2fold_epoch_29.pickle',\n",
              " 'rgb2fold_epoch_30.pickle',\n",
              " 'rgb2fold_epoch_31.pickle',\n",
              " 'rgb2fold_epoch_32.pickle',\n",
              " 'rgb2fold_epoch_33.pickle',\n",
              " 'rgb2fold_epoch_34.pickle',\n",
              " 'rgb2fold_epoch_35.pickle',\n",
              " 'rgb2fold_epoch_36.pickle',\n",
              " 'rgb2fold_epoch_37.pickle',\n",
              " 'rgb2fold_epoch_38.pickle',\n",
              " 'rgb2fold_epoch_39.pickle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwKcWHfXA-Hm",
        "outputId": "207a1b15-2187-43aa-aaa9-a7197b70bdd0"
      },
      "source": [
        "os.listdir(EXP_PKL_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rgb2fold_epoch_1.pickle',\n",
              " 'rgb2fold_epoch_2.pickle',\n",
              " 'rgb2fold_epoch_3.pickle',\n",
              " 'rgb2fold_epoch_4.pickle',\n",
              " 'rgb2fold_epoch_5.pickle',\n",
              " 'rgb2fold_epoch_6.pickle',\n",
              " 'rgb2fold_epoch_7.pickle',\n",
              " 'rgb2fold_epoch_8.pickle',\n",
              " 'rgb2fold_epoch_9.pickle',\n",
              " 'rgb2fold_epoch_10.pickle',\n",
              " 'rgb2fold_epoch_11.pickle',\n",
              " 'rgb2fold_epoch_12.pickle',\n",
              " 'rgb2fold_epoch_13.pickle',\n",
              " 'rgb2fold_epoch_14.pickle',\n",
              " 'rgb2fold_epoch_15.pickle',\n",
              " 'rgb2fold_epoch_16.pickle',\n",
              " 'rgb2fold_epoch_17.pickle',\n",
              " 'rgb2fold_epoch_18.pickle',\n",
              " 'rgb2fold_epoch_19.pickle',\n",
              " 'rgb2fold_epoch_20.pickle',\n",
              " 'rgb2fold_epoch_21.pickle',\n",
              " 'rgb2fold_epoch_22.pickle',\n",
              " 'rgb2fold_epoch_23.pickle',\n",
              " 'rgb2fold_epoch_24.pickle',\n",
              " 'rgb2fold_epoch_25.pickle',\n",
              " 'rgb2fold_epoch_26.pickle',\n",
              " 'rgb2fold_epoch_27.pickle',\n",
              " 'rgb2fold_epoch_28.pickle',\n",
              " 'rgb2fold_epoch_29.pickle',\n",
              " 'rgb2fold_epoch_30.pickle',\n",
              " 'rgb2fold_epoch_31.pickle',\n",
              " 'rgb2fold_epoch_32.pickle',\n",
              " 'rgb2fold_epoch_33.pickle',\n",
              " 'rgb2fold_epoch_34.pickle',\n",
              " 'rgb2fold_epoch_35.pickle',\n",
              " 'rgb2fold_epoch_36.pickle',\n",
              " 'rgb2fold_epoch_37.pickle',\n",
              " 'rgb2fold_epoch_38.pickle',\n",
              " 'rgb2fold_epoch_39.pickle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzFEV_MCBuxK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}