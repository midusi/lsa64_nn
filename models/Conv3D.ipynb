{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Conv3D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jiQK97xEy7f"
      },
      "source": [
        "CORRER ESTA CELDA PARA CONECTAR A DRIVE\n",
        "* elegir cuenta\n",
        "* poner permitir\n",
        "* copiar el token y pegar al input\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znuik6QlSIlx",
        "outputId": "08864722-8cd2-4523-8087-bc08aafcccdb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owHW6xBk6XG0"
      },
      "source": [
        "CALLBACKS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE5BbQBK4FJe"
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "class MyCheckpoint(Callback):\n",
        "  def __init__(self, start_epoch, exp_folder, weights_path, pickles_path, EXP_NAME, model_name):\n",
        "    super(MyCheckpoint, self).__init__()\n",
        "    self.current_epoch = start_epoch + 1  \n",
        "    self.timestamps = {}\n",
        "    self.exp_folder = exp_folder\n",
        "    self.weights_path = weights_path\n",
        "    self.pickles_path = pickles_path\n",
        "    self.EXP_NAME = EXP_NAME  \n",
        "    self.model_name = model_name\n",
        "    self.model_training_time = 0\n",
        "    self.start_batch = 0\n",
        "    self.end_batch = 0\n",
        "\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    self.timestamps[self.current_epoch] = {}   \n",
        "    self.timestamps[self.current_epoch][\"start\"] = datetime.now().time().strftime(\"%H:%M\")  \n",
        "\n",
        "  def on_test_begin(self, logs=None):    \n",
        "    self.model.save_weights(os.path.join(self.weights_path+'{}_epoch_{}.h5'.format(self.model_name,self.current_epoch)))          \n",
        "    self.timestamps[self.current_epoch][\"end\"] = datetime.now().time().strftime(\"%H:%M\")  # PASAR ARRIBA POR EL SAVE\n",
        "    self.timestamps[self.current_epoch][\"train_time\"] = self.model_training_time \n",
        "    with open(os.path.join(self.exp_folder, f\"timestamps_epoch{self.current_epoch}.pickle\"), 'wb') as fd:\n",
        "      pickle.dump(self.timestamps, fd)\n",
        "    self.model_training_time = 0\n",
        "\n",
        "  def on_train_batch_begin(self, batch, logs=None):\n",
        "      self.start_batch = time.time()\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "      self.end_batch = time.time()\n",
        "      self.model_training_time+= (self.end_batch - self.start_batch)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):            \n",
        "    with open(os.path.join(self.pickles_path+'{}_epoch_{}.pickle'.format(self.model_name,self.current_epoch)), 'wb') as fd:\n",
        "      pickle.dump(logs, fd)\n",
        "    self.current_epoch = self.current_epoch + 1    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOHH53JEqR3"
      },
      "source": [
        "MUY IMPORTANTE EL FOLD QUE ELIJAS ACA TIENE QUE COINCIDIR CON LA CARPETA QUE ELIJAS DESPUES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7WmBwjv6GG9"
      },
      "source": [
        "import os\n",
        "#Esto es lo que cambia\n",
        "FOLD_NAME = '0fold'\n",
        "\n",
        "#Esto es fijo\n",
        "DATASET = 'Multisampling16'\n",
        "MODEL_NAME = 'rgb'+FOLD_NAME\n",
        "\n",
        "basePath = \"/content/gdrive/My Drive/Informática/Tesina/\"\n",
        "videos_path = basePath+'Videos/'+DATASET\n",
        "fls = os.listdir(videos_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQtTh8hiPEat"
      },
      "source": [
        "OBJETO EXPERIMENTO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msLIc5_rhzmy"
      },
      "source": [
        "import os\n",
        "\n",
        "from keras.initializers import HeNormal\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.activations import elu\n",
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "\n",
        "basePath = \"/content/gdrive/My Drive/Informática/Tesina/\"\n",
        "\n",
        "datasets = {            \n",
        "            \"Multisampling32\":{\"path\": basePath+\"Videos/Multisampling32/\",\"num_frames\": 32,\"files_posfix\":\"Multisampling32\"},  \n",
        "            \"Multisampling16\":{\"path\": basePath+\"Videos/Multisampling16/\",\"num_frames\": 16, \"files_posfix\":\"Multisampling16\"},  \n",
        "            \"Multisampling8\":{\"path\": basePath+\"Videos/Multisampling8/\",\"num_frames\": 8, \"files_posfix\":\"Multisampling8\"},  \n",
        "            \"SimpleSampling32\":{\"path\": basePath+\"Videos/SimpleSampling32/\",\"num_frames\": 32, \"files_posfix\":\"SimpleSampling32\"},              \n",
        "  }\n",
        "\n",
        "class Experiment():\n",
        "\n",
        "  def __init__(self, exp_name, model_name):\n",
        "    #Logging and checkpoints\n",
        "    self.EXP_NAME = exp_name\n",
        "    self.model_name = model_name\n",
        "    self.exp_folder = os.path.join(basePath+\"Experimentos/Conv3D\", exp_name, model_name)\n",
        "    self.weights_path = os.path.join(self.exp_folder,\"Weights/\")\n",
        "    self.pickles_path = os.path.join(self.exp_folder,\"Pickles/\")\n",
        "    if (os.path.exists(self.exp_folder)):           \n",
        "      weights = os.listdir(self.weights_path)\n",
        "      # get the latest epoch trained\n",
        "      try:\n",
        "        self.current_epoch = sorted(list(map(lambda x: int(x.split('_')[-1].split('.')[0]), weights)), reverse=True)[0] \n",
        "      except (TypeError, IndexError):\n",
        "        self.current_epoch = 0      \n",
        "    else:      \n",
        "      os.mkdir(self.exp_folder)      \n",
        "      os.mkdir(self.weights_path)\n",
        "      os.mkdir(self.pickles_path)\n",
        "      self.current_epoch = 0\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir=os.path.join(self.exp_folder, 'logs'), histogram_freq=1)\n",
        "    early = EarlyStopping(monitor='val_accuracy', patience=12)\n",
        "    self.testing = False\n",
        "    #Data specs\n",
        "    data_dict = datasets[DATASET]\n",
        "    self.fold_name = FOLD_NAME\n",
        "    self.files_posfix = DATASET\n",
        "    self.names_path = os.path.join(basePath+\"/DatasetsNames\", DATASET)\n",
        "    self.videos_path = data_dict['path']\n",
        "    self.num_frames = data_dict['num_frames']\n",
        "    self.height = 128 #data_dict['height']\n",
        "    self.width = 128 #data_dict['width']\n",
        "    self.channels = 1\n",
        "    #Model specs\n",
        "    self.activation = elu\n",
        "    self.initializer = HeNormal\n",
        "    self.l1=1e-5\n",
        "    self.l2=1e-5\n",
        "    self.regularizer = l1_l2(l1=self.l1, l2=self.l2)\n",
        "    self.conv_blocks = [\n",
        "                        {'filters':16, 'kernel_size':3, 'pool_size':3},\n",
        "                        {'filters':32, 'kernel_size':3, 'pool_size':3},\n",
        "                        {'filters':32, 'kernel_size':3, 'pool_size':3},  \n",
        "                        {'filters':64, 'kernel_size':3, 'pool_size':3},  \n",
        "                    ]\n",
        "\n",
        "\n",
        "    self.batch_size = 32\n",
        "    self.epochs_to_train = 20-self.current_epoch\n",
        "    self.lr = 0.001\n",
        "    self.callbacks = [MyCheckpoint(self.current_epoch, self.exp_folder, self.weights_path, self.pickles_path, self.EXP_NAME, self.model_name), tensorboard, early]\n",
        "    \n",
        "      \n",
        "\n",
        "  def save_resume(self):\n",
        "    with open(os.path.join(self.exp_folder,'exp_resume.txt'), 'w') as f:\n",
        "      stri = \"\"\n",
        "      settings = [a for a in dir(self) if not a.startswith('__')]\n",
        "      for setting in settings:\n",
        "        stri = stri + setting+\": \"+str(getattr(experiment, setting))+'\\n'\n",
        "      f.writelines(stri)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PZtHwd7OsN6"
      },
      "source": [
        "MODELOS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGDPLWIeOr4H"
      },
      "source": [
        "\n",
        "from keras.layers import Input, Conv3D, BatchNormalization, MaxPooling3D, Dense, GlobalAveragePooling3D\n",
        "from keras import Model\n",
        "from keras.initializers import HeNormal\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "def conv_3d_block(input,index,filters, kernel_size, activation,initializer,regularizer, pool_size):\n",
        "  conv = Conv3D(\n",
        "      filters=filters, \n",
        "      kernel_size=(kernel_size,kernel_size, kernel_size),      \n",
        "      activation=activation, \n",
        "      kernel_initializer=initializer(),\n",
        "      kernel_regularizer=regularizer, \n",
        "      padding='same',      \n",
        "      name=f\"Conv_{index}\")(input)\n",
        "  batch = BatchNormalization(name=f\"Batch_{index}\")(conv)\n",
        "  pooling = MaxPooling3D(pool_size=(pool_size,pool_size,pool_size), padding='same',name=f\"3dPool_{index}\")(batch)  \n",
        "  return pooling\n",
        "\n",
        "def last_conv_3d_block(input,filters, kernel_size, activation,initializer,regularizer, pool_size):\n",
        "  conv = Conv3D(\n",
        "      filters=filters, \n",
        "      kernel_size=(kernel_size,kernel_size, kernel_size),      \n",
        "      activation=activation, \n",
        "      kernel_initializer=initializer(),\n",
        "      kernel_regularizer=regularizer, \n",
        "      padding='same',      \n",
        "      name=\"final_conv\")(input) #La proxima entonces toma solo lo ultimo\n",
        "  batch = BatchNormalization(name=\"final_batch\")(conv)\n",
        "  avg_pooling = GlobalAveragePooling3D()(batch)  \n",
        "  return avg_pooling\n",
        "\n",
        "def conv_stack(input, experiment):\n",
        "  stack = input\n",
        "  for i in range(0,len(experiment.conv_blocks)-1):\n",
        "    stack = conv_3d_block(stack,i,\n",
        "      experiment.conv_blocks[i]['filters'],\n",
        "      experiment.conv_blocks[i]['kernel_size'],\n",
        "      experiment.activation,\n",
        "      experiment.initializer,\n",
        "      experiment.regularizer,\n",
        "      experiment.conv_blocks[i]['pool_size']\n",
        "    )\n",
        "  stack = last_conv_3d_block(stack,\n",
        "      experiment.conv_blocks[-1]['filters'],\n",
        "      experiment.conv_blocks[-1]['kernel_size'],\n",
        "      experiment.activation,\n",
        "      experiment.initializer,\n",
        "      experiment.regularizer,\n",
        "      experiment.conv_blocks[-1]['pool_size']\n",
        "  )\n",
        "  return stack\n",
        "\n",
        "def Conv3D_model(experiment,categories=64):\n",
        "  input = Input(shape=(experiment.num_frames, experiment.height, experiment.width,experiment.channels))\n",
        "  stack = conv_stack(input, experiment)\n",
        "  output = Dense(categories, activation='softmax')(stack)\n",
        "  model = Model(inputs=input, outputs=output, name=experiment.model_name)  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XbpfKIzO3wk"
      },
      "source": [
        "GENERADOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bheU5Tjq49Di"
      },
      "source": [
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "def generator_cropped(videos_path,samples, batch_size, categories=64):\n",
        "  \"\"\"\n",
        "    samples serian los nombres de archivos\n",
        "  \"\"\"\n",
        "  height_upper_limit = int(1080/4)\n",
        "  width_upper_limit = int(1920/4)\n",
        "  height_to_crop = int(height_upper_limit*3)\n",
        "  width_to_crop = int(width_upper_limit*3)\n",
        "  w = experiment.width\n",
        "  h = experiment.height\n",
        "  num_samples = len(samples) \n",
        "  while True:\n",
        "    for offset in range(0,num_samples, batch_size):\n",
        "      batch_samples = samples[offset:offset+batch_size]\n",
        "      X_train = []\n",
        "      y_train = []\n",
        "      for batch_sample in batch_samples:\n",
        "        fname, w_limit, h_limit = batch_sample.split(\" \")\n",
        "        w_limit, h_limit = int(w_limit), int(h_limit)\n",
        "        vidcap = cv2.VideoCapture(videos_path+fname)      \n",
        "        #vidcap = imageio.get_reader(videos_path+batch_sample)\n",
        "        frames = []\n",
        "        suc, frame = vidcap.read()                \n",
        "        #for i, frame in enumerate(vidcap):\n",
        "        while suc: \n",
        "          frame = frame[h_limit:h_limit+height_to_crop,w_limit:w_limit+width_to_crop,:]\n",
        "          frame = cv2.resize(frame, (w,h), interpolation=cv2.INTER_AREA)\n",
        "          frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if (experiment.channels==3) else np.expand_dims(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),2)\n",
        "          frame = frame.astype(np.float32)\n",
        "          frame /= 255.0          \n",
        "\n",
        "          frames.append(frame)\n",
        "          suc, frame = vidcap.read()                \n",
        "        X_train.append(frames)        \n",
        "        cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "        y_train.append(to_categorical(cat, num_classes=categories))      \n",
        "      try:\n",
        "        X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "      except:\n",
        "        print(\"Trying again...\")\n",
        "        try:\n",
        "          X_train = np.asarray(X_train, dtype=np.float32)\n",
        "        except:\n",
        "          print(\"failed reading\")      \n",
        "      y_train = np.asarray(y_train)      \n",
        "      yield X_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCmEPlGHULzg"
      },
      "source": [
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "def generator(videos_path,samples, batch_size, categories=64):\n",
        "  \"\"\"\n",
        "    samples serian los nombres de archivos\n",
        "  \"\"\"\n",
        "  w = experiment.width\n",
        "  h = experiment.height\n",
        "  num_samples = len(samples) \n",
        "  while True:\n",
        "    for offset in range(0,num_samples, batch_size):\n",
        "      batch_samples = samples[offset:offset+batch_size]\n",
        "      X_train = []\n",
        "      y_train = []\n",
        "      for batch_sample in batch_samples:\n",
        "        vidcap = cv2.VideoCapture(videos_path+batch_sample)      \n",
        "        #vidcap = imageio.get_reader(videos_path+batch_sample)\n",
        "        frames = []\n",
        "        suc, frame = vidcap.read()                \n",
        "        #for i, frame in enumerate(vidcap):\n",
        "        while suc: \n",
        "          frame = cv2.resize(frame, (w,h), interpolation=cv2.INTER_AREA)\n",
        "          \n",
        "          \n",
        "          frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if (experiment.channels==3) else np.expand_dims(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),2)\n",
        "          \n",
        "          \n",
        "          frame = frame.astype(np.float32)\n",
        "          \n",
        "          \n",
        "          frame /= 255.0          \n",
        "          \n",
        "          \n",
        "\n",
        "          frames.append(frame)\n",
        "          suc, frame = vidcap.read()        \n",
        "        \n",
        "        X_train.append(frames)        \n",
        "        cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "        y_train.append(to_categorical(cat, num_classes=categories))      \n",
        "      try:\n",
        "        X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "      except:\n",
        "        print(\"Trying again...\")\n",
        "        try:\n",
        "          X_train = np.asarray(X_train, dtype=np.float32)\n",
        "        except:\n",
        "          print(\"failed reading\")      \n",
        "      y_train = np.asarray(y_train)      \n",
        "      yield X_train, y_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAuGP69bKutD"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications import mobilenet\n",
        "from keras.utils import Sequence\n",
        "import math \n",
        "class SequenceGenerator(Sequence):\n",
        "  def __init__(self, videos_path, samples,batch_size, width, height, channels):\n",
        "    self.batch_size = batch_size\n",
        "    self.videos_path = videos_path\n",
        "    self.samples = samples\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.channels = channels\n",
        "    self.len = len(os.listdir(videos_path))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch_samples = self.samples[idx * self.batch_size:(idx+1)*self.batch_size]\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for batch_sample in batch_samples:      \n",
        "      vidcap = cv2.VideoCapture(os.path.join(self.videos_path,batch_sample))              \n",
        "      frames = []\n",
        "      suc, frame = vidcap.read()              \n",
        "      while suc: \n",
        "        frame = cv2.resize(frame, (self.width,self.height), interpolation=cv2.INTER_AREA)\n",
        "        frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if (self.channels==3) else np.expand_dims(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),2)\n",
        "        frame = frame.astype(np.float32)\n",
        "        frame /= 255.0          \n",
        "        frames.append(frame)\n",
        "        suc, frame = vidcap.read()      \n",
        "      \n",
        "      X_train.append(frames)        \n",
        "      cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "      y_train.append(to_categorical(cat, num_classes=64))      \n",
        "        \n",
        "    X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "    y_train = np.asarray(y_train)      \n",
        "    return X_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-1OI0yMKLEQ"
      },
      "source": [
        "INICIALIZACION:\n",
        "\n",
        "\n",
        "*   CORRES LA CELDA\n",
        "*   ELEGIS LA OPCION NRO 3 (MULTISAMPLING32)\n",
        "*   SI ESTAS USANDO UN FOLD NUEVO PONES: y\n",
        "  * ESCRIBIS 'rgb{FOLDNAME}fold' [i.e 'rgb2fold']\n",
        "*   SI ESTAS UN FOLD QUE YA EMPEZASTE A ENTRENAR PONES: n\n",
        "  * ELEGIS EL NUMERO DE FOLD QUE ESTES ENTRENANDO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ls4LIuKNKM",
        "outputId": "19f4417c-4372-4851-ec76-89c0ccd1834e"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "datasetsFolders = os.listdir(\"/content/gdrive/My Drive/Informática/Tesina/Experimentos/Conv3D\")\n",
        "for d in datasetsFolders:\n",
        "  print(f\"{datasetsFolders.index(d)} - {d}\")\n",
        "print(\"Ingrese el numero del Dataset: \")\n",
        "#chosen = input()\n",
        "#exp_name = datasetsFolders[int(chosen)]\n",
        "print(DATASET)\n",
        "exp_name = DATASET\n",
        "#if not os.path.exists(os.path.join(\"/content/gdrive/My Drive/Informática/Tesina/Experimentos/Conv3D\", )):\n",
        "#  os.mkdir(os.path.join(\"/content/gdrive/My Drive/Informática/Tesina/Experimentos/Conv3D\", exp_name))\n",
        "print(\"¿Nuevo experimento? [y|n]\" )\n",
        "print('y')\n",
        "print('Model name: '+MODEL_NAME)\n",
        "#if input()=='y':\n",
        "#  print(\"Experimentos Previos:\")\n",
        "#  print(os.listdir(os.path.join(\"/content/gdrive/My Drive/Informática/Tesina/Experimentos/Conv3D\", exp_name)))\n",
        "#  print(\"Nombre de modelo [default: Baseline]: \")\n",
        "#  model_name = input()\n",
        "#  model_name = \"baseline\" if model_name == '' else model_name\n",
        "#  if model_name in os.listdir(os.path.join(\"/content/gdrive/My Drive/Informática/Tesina/Experimentos/Conv3D\", exp_name)):\n",
        "#    print(\"Ya esta el experimento, empiece de vuelta\")\n",
        "#    raise Exception\n",
        "#    \n",
        "#else:\n",
        "#  experiments = os.listdir(os.path.join(\"/content/gdrive/My Drive/Informática/Tesina/Experimentos/Conv3D\", exp_name))\n",
        "#  for d in experiments:\n",
        "#    print(f\"{experiments.index(d)} - {d}\")\n",
        "#  print(\"Ingrese el numero del Modelo: \")\n",
        "#  chosen = input()  \n",
        "#  model_name = experiments[int(chosen)]\n",
        "  \n",
        "\n",
        "\n",
        "experiment = Experiment(exp_name, MODEL_NAME)\n",
        "experiment.save_resume()\n",
        "weights_path = experiment.weights_path\n",
        "pickles_path = experiment.pickles_path\n",
        "videos_path = experiment.videos_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 - Brasil\n",
            "1 - Multisampling16\n",
            "2 - Multisampling8\n",
            "3 - Multisampling32\n",
            "4 - SimpleSampling32\n",
            "Ingrese el numero del Dataset: \n",
            "SimpleSampling32\n",
            "¿Nuevo experimento? [y|n]\n",
            "y\n",
            "Model name: gray0fold\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIaJWpOR6XGu"
      },
      "source": [
        "TRAIN VAL TEST SPLIT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbRYPG6nKN8C"
      },
      "source": [
        "\n",
        "with open(os.path.join(experiment.names_path,f'train_{experiment.fold_name}_{experiment.files_posfix}.txt'), 'r') as f:\n",
        "  train = f.read().split('\\n')\n",
        "with open(os.path.join(experiment.names_path,f'val_{experiment.fold_name}_{experiment.files_posfix}.txt'), 'r') as f:\n",
        "  val = f.read().split('\\n')\n",
        "with open(os.path.join(experiment.names_path,f'test_{experiment.fold_name}_{experiment.files_posfix}.txt'), 'r') as f:\n",
        "  test = f.read().split('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAC-AdNFKOUx"
      },
      "source": [
        "ENTRENAMIENTO\n",
        "CORRER ESTA CELDA 2 VECES:\n",
        "* PONES A CORRER\n",
        "* FALLA\n",
        "* VOLVER A CORRER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6JoWg3rGIhU",
        "outputId": "ece814c5-ee7a-4695-da77-94f1b879c967"
      },
      "source": [
        "\n",
        "batch_size = experiment.batch_size\n",
        "\n",
        "if f\"{experiment.model_name}.pickle\" in os.listdir(experiment.exp_folder):  \n",
        "  with open(os.path.join(experiment.exp_folder, f\"{experiment.model_name}.pickle\"), 'rb') as pklfile:\n",
        "    conf = pickle.load(pklfile)    \n",
        "    model = Model.from_config(conf)\n",
        "else:\n",
        "  model = Conv3D_model(experiment)\n",
        "  with open(os.path.join(experiment.exp_folder, f\"{experiment.model_name}.pickle\"), 'wb') as pklfile:\n",
        "    conf = model.get_config()\n",
        "    pickle.dump(conf, pklfile)\n",
        "    with open(os.path.join(experiment.exp_folder, f'{experiment.model_name}.txt'), 'w') as f:\n",
        "      model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "if '{}_epoch_{}.h5'.format(experiment.model_name, experiment.current_epoch) in os.listdir(weights_path):\n",
        "  model.load_weights(os.path.join(experiment.weights_path,'{}_epoch_{}.h5'.format(experiment.model_name, experiment.current_epoch)))\n",
        "\n",
        "adam = Adam(learning_rate=experiment.lr)\n",
        "model.compile(optimizer=adam,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "if experiment.testing:\n",
        "  print(\"Estas seguro que queres testear? [y/n]\")\n",
        "  if (input() == 'y'):\n",
        "    eval_dict = model.evaluate(\n",
        "        generator(experiment.videos_path,test, batch_size)\n",
        "      , steps=int(len(test)/batch_size)\n",
        "      , batch_size=experiment.batch_size\n",
        "      , verbose = 1\n",
        "      , return_dict=True\n",
        "    )\n",
        "    with open(os.path.join(experiment.exp_folder, f'evaluation_epoch{experiment.current_epoch}_toLength.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(eval_dict, pkl)\n",
        "  else: \n",
        "    print(\"Cambia el experimento entonces en experiment.testing\")\n",
        "else:\n",
        "  train_gen = SequenceGenerator(videos_path, train, batch_size, experiment.width, experiment.height, experiment.channels)    \n",
        "  val_gen = SequenceGenerator(videos_path, val, batch_size, experiment.width, experiment.height, experiment.channels)    \n",
        "  history = model.fit(\n",
        "    train_gen\n",
        "    , steps_per_epoch=int(len(train)/batch_size)\n",
        "    , validation_data = val_gen\n",
        "    ,validation_steps = int(len(val)/batch_size)\n",
        "    , epochs = experiment.epochs_to_train\n",
        "    , verbose = 1\n",
        "    , shuffle = False    \n",
        "    , callbacks=list(experiment.callbacks)\n",
        "    , use_multiprocessing=True\n",
        "    , workers=4\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"noreg0fold\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 64, 64, 1)]   0         \n",
            "_________________________________________________________________\n",
            "Conv_0 (Conv3D)              (None, 32, 64, 64, 16)    448       \n",
            "_________________________________________________________________\n",
            "Batch_0 (BatchNormalization) (None, 32, 64, 64, 16)    64        \n",
            "_________________________________________________________________\n",
            "3dPool_0 (MaxPooling3D)      (None, 16, 32, 32, 16)    0         \n",
            "_________________________________________________________________\n",
            "Conv_1 (Conv3D)              (None, 16, 32, 32, 32)    13856     \n",
            "_________________________________________________________________\n",
            "Batch_1 (BatchNormalization) (None, 16, 32, 32, 32)    128       \n",
            "_________________________________________________________________\n",
            "3dPool_1 (MaxPooling3D)      (None, 8, 16, 16, 32)     0         \n",
            "_________________________________________________________________\n",
            "Conv_2 (Conv3D)              (None, 8, 16, 16, 32)     27680     \n",
            "_________________________________________________________________\n",
            "Batch_2 (BatchNormalization) (None, 8, 16, 16, 32)     128       \n",
            "_________________________________________________________________\n",
            "3dPool_2 (MaxPooling3D)      (None, 4, 8, 8, 32)       0         \n",
            "_________________________________________________________________\n",
            "final_conv (Conv3D)          (None, 4, 8, 8, 64)       55360     \n",
            "_________________________________________________________________\n",
            "final_batch (BatchNormalizat (None, 4, 8, 8, 64)       256       \n",
            "_________________________________________________________________\n",
            "global_average_pooling3d (Gl (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "=================================================================\n",
            "Total params: 102,080\n",
            "Trainable params: 101,792\n",
            "Non-trainable params: 288\n",
            "_________________________________________________________________\n",
            "Epoch 1/6\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.9838 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 997s 14s/step - loss: 0.2668 - accuracy: 0.9837 - val_loss: 1.3802 - val_accuracy: 0.5826\n",
            "Epoch 2/6\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9948 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 906s 13s/step - loss: 0.1826 - accuracy: 0.9947 - val_loss: 1.4381 - val_accuracy: 0.5915\n",
            "Epoch 3/6\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9969 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 903s 13s/step - loss: 0.1361 - accuracy: 0.9969 - val_loss: 1.3324 - val_accuracy: 0.6183\n",
            "Epoch 4/6\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9974 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 905s 13s/step - loss: 0.1010 - accuracy: 0.9974 - val_loss: 1.2455 - val_accuracy: 0.6138\n",
            "Epoch 5/6\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9987 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 908s 13s/step - loss: 0.0766 - accuracy: 0.9987 - val_loss: 1.6348 - val_accuracy: 0.5290\n",
            "Epoch 6/6\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9996 WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "66/66 [==============================] - 907s 13s/step - loss: 0.0596 - accuracy: 0.9996 - val_loss: 2.3229 - val_accuracy: 0.4286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4HpsQeUoE0h"
      },
      "source": [
        "SI SE TRABA CORRER ESTO PARA SABER QUE HAY QUE HACER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pJpw4I4fanS",
        "outputId": "3ff37224-9672-40d5-9882-ce742f1676f2"
      },
      "source": [
        "try:\n",
        "  print(\"Actualizando epoca actual...\")\n",
        "  print('Previo: '+str(experiment.current_epoch))\n",
        "  weights = os.listdir(experiment.weights_path)\n",
        "  experiment.current_epoch = sorted(list(map(lambda x: int(x.split('_')[-1].split('.')[0]), weights)), reverse=True)[0] \n",
        "  experiment.epochs_to_train = 20-experiment.current_epoch\n",
        "  print('Actual: '+str(experiment.current_epoch))\n",
        "  print('Epocas a entrenar: '+str(experiment.epochs_to_train))\n",
        "  print('SE PUEDE VOLVER A EJECUTAR LA CELDA DE ENTRENAMIENTO!')\n",
        "except:\n",
        "  print(\"ERROR!\\nCORRER DE VUELTA DESDE LA PRIMER CELDA (MONTADO DE DRIVE)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actualizando epoca actual...\n",
            "ERROR!\n",
            "CORRER DE VUELTA DESDE LA PRIMER CELDA (MONTADO DE DRIVE)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdwwg9Fjp3EX"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications import mobilenet\n",
        "from keras.utils import Sequence\n",
        "import math \n",
        "class SequenceGenerator(Sequence):\n",
        "  def __init__(self, videos_path, samples,batch_size, width, height, channels):\n",
        "    self.batch_size = batch_size\n",
        "    self.videos_path = videos_path\n",
        "    self.samples = samples\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.channels = channels\n",
        "    self.len = len(os.listdir(videos_path))\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    batch_samples = self.samples[idx * self.batch_size:(idx+1)*self.batch_size]\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for batch_sample in batch_samples:      \n",
        "      vidcap = cv2.VideoCapture(os.path.join(self.videos_path,batch_sample))              \n",
        "      frames = []\n",
        "      suc, frame = vidcap.read()              \n",
        "      while suc: \n",
        "        frame = cv2.resize(frame, (self.width,self.height), interpolation=cv2.INTER_AREA)\n",
        "        frame =  cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if (self.channels==3) else np.expand_dims(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),2)\n",
        "        frame = frame.astype(np.float32)\n",
        "        frame /= 255.0          \n",
        "        frames.append(frame)\n",
        "        suc, frame = vidcap.read()      \n",
        "      \n",
        "      X_train.append(frames)        \n",
        "      cat = int(batch_sample.split('_')[0])#  Los mp4 estan categorizados de 1 a 64 y los .avi de 0 a 64, asi que no necesito restar 1        \n",
        "      y_train.append(to_categorical(cat, num_classes=64))      \n",
        "        \n",
        "    X_train = np.asarray(X_train, dtype=np.float32)      \n",
        "    y_train = np.asarray(y_train)      \n",
        "    return X_train, y_train\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StArHiMYDIg9"
      },
      "source": [
        "DATASET = 'Multisampling8'\n",
        "import numpy as np\n",
        "import pickle\n",
        "import functools as ft\n",
        "from keras import Model\n",
        "from keras.optimizers import Adam\n",
        "def addHistories(h1, h2):    \n",
        "    for key in [\"accuracy\", \"loss\", \"val_loss\", \"val_accuracy\"]:\n",
        "        h2[key].append(h1[key]) #ya no es una lista asi que puedo acceder a la metrica directo \n",
        "    return h2\n",
        "\n",
        "basePath = \"/content/gdrive/My Drive/Informática/Tesina/\"\n",
        "baseFolder = basePath+'Experimentos/Conv3D/'+DATASET\n",
        "namesPath = basePath+'DatasetsNames/'+DATASET\n",
        "videos_path = basePath+'Videos/'+DATASET\n",
        "best_epochs = {}\n",
        "#mean_acc = {'rgb':[], 'rgb64_':[],'gray':[], 'gray64_':[]}\n",
        "#mean_val_acc = {'rgb':[], 'rgb64_':[],'gray':[], 'gray64_':[]}\n",
        "mean_acc = {'8':[], '16':[], '32':[]}\n",
        "videos_path = {'8':basePath+'Videos/Multisampling8/','16':basePath+'Videos/Multisampling16/', '32':basePath+'Videos/Multisampling32/' }\n",
        "#model_settings = {'rgb':{'res':128, 'colours':3}, 'rgb64_':{'res':64, 'colours':3},'gray':{'res':128, 'colours':1}, 'gray64_':{'res':64, 'colours':1}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scCbl6-XDC0E",
        "outputId": "fd1d4614-a74e-4150-e30c-88d46df0c010"
      },
      "source": [
        "\n",
        "for fold in ['0fold', '1fold', '2fold','3fold', '4fold']:\n",
        "  #for model_type in ['rgb', 'rgb64_','gray', 'gray64_']:\n",
        "    modelName = 'rgb'+fold\n",
        "    expFolder = os.path.join(baseFolder,modelName)\n",
        "    if f\"{modelName}.pickle\" in os.listdir(expFolder):  \n",
        "      with open(os.path.join(expFolder, f\"{modelName}.pickle\"), 'rb') as pklfile:\n",
        "        conf = pickle.load(pklfile)    \n",
        "        model = Model.from_config(conf)\n",
        "    else:\n",
        "      print(\"no encuentro modelo\")\n",
        "    adam = Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=adam,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    EXP_PKL_PATH = os.path.join(expFolder, 'Pickles')  \n",
        "    pkl_files = os.listdir(EXP_PKL_PATH)\n",
        "    fls_s = sorted(pkl_files, key=lambda x: int(x.split('_')[-1].split('.')[0])) #Exp_18_epoch_2.pickle\n",
        "\n",
        "    pkls = []\n",
        "    for pkl_file in fls_s:\n",
        "        with open(os.path.join(EXP_PKL_PATH ,pkl_file), 'rb') as pkl:\n",
        "            h_temp = pickle.load(pkl)        \n",
        "            #print(h_temp)\n",
        "            if 'val_loss' in h_temp.keys():\n",
        "              pkls.append(h_temp)\n",
        "\n",
        "\n",
        "    init_dict = {}\n",
        "    for k in [\"accuracy\", \"loss\", \"val_loss\", \"val_accuracy\"]:\n",
        "      init_dict[k] = []\n",
        "    hs = ft.reduce(lambda a,b: addHistories(b,a), pkls, init_dict)\n",
        "    best_epoch = np.array(hs['val_accuracy']).argmax()+1\n",
        "    best_epochs[fold] = (best_epoch,hs['val_accuracy'][best_epoch-1])\n",
        "    #mean_val_acc[model_type].append((best_epoch,hs['val_accuracy'][best_epoch-1]))\n",
        "    model.load_weights(os.path.join(expFolder,'Weights','{}_epoch_{}.h5'.format(modelName, best_epoch)))\n",
        "    print(modelName)\n",
        "    print(f\"Entreno {len(hs['val_accuracy'])}\")\n",
        "    print((best_epoch,hs['val_accuracy'][best_epoch-1]))\n",
        "    with open(os.path.join(namesPath,f'test_{fold}_{DATASET}.txt'), 'r') as f:\n",
        "      test = f.read().split('\\n')  \n",
        "    for num_frames in ['8', '16', '32']:\n",
        "      path = videos_path[num_frames]\n",
        "      #gen = SequenceGenerator(path, test, 32, model_settings[model_type]['res'], model_settings[model_type]['res'], model_settings[model_type]['colours'])    \n",
        "      gen = SequenceGenerator(path, test, 32, 128, 128, 3)    \n",
        "      eval_dict = model.evaluate(\n",
        "            gen\n",
        "          , steps=int(len(test)/32)\n",
        "          , batch_size=32\n",
        "          , verbose = 1\n",
        "          , return_dict=True\n",
        "          ,use_multiprocessing=True\n",
        "          ,workers=4\n",
        "        )\n",
        "      #mean_acc[model_type].append(eval_dict['accuracy'])\n",
        "      mean_acc[num_frames].append(eval_dict['accuracy'])\n",
        "      with open(os.path.join(expFolder, f'evaluation_epoch{best_epoch}_frames.pickle'), 'wb') as pkl:\n",
        "        pickle.dump(eval_dict, pkl)    \n",
        "\n",
        "with open(os.path.join(baseFolder, f'tests.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(mean_acc, pkl)\n",
        "\n",
        "with open(os.path.join(baseFolder, f'bestEpochs.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(best_epochs, pkl)        \n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rgb0fold\n",
            "Entreno 20\n",
            "(18, 0.96875)\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 369s 4s/step - loss: 0.1721 - accuracy: 0.9605\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 571s 7s/step - loss: 4.2002 - accuracy: 0.3133\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 1033s 13s/step - loss: 17.6184 - accuracy: 0.0996\n",
            "rgb1fold\n",
            "Entreno 20\n",
            "(20, 0.981249988079071)\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 364s 5s/step - loss: 0.1017 - accuracy: 0.9766\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 565s 7s/step - loss: 4.2251 - accuracy: 0.3941\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 1031s 13s/step - loss: 9.7188 - accuracy: 0.1668\n",
            "rgb2fold\n",
            "Entreno 20\n",
            "(19, 0.9937499761581421)\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 358s 4s/step - loss: 0.0809 - accuracy: 0.9868\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 561s 7s/step - loss: 4.7065 - accuracy: 0.3816\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 1035s 13s/step - loss: 32.5067 - accuracy: 0.0270\n",
            "rgb3fold\n",
            "Entreno 20\n",
            "(20, 0.981249988079071)\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 360s 4s/step - loss: 0.0654 - accuracy: 0.9914\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 557s 7s/step - loss: 18.2468 - accuracy: 0.1133\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 1031s 13s/step - loss: 80.3471 - accuracy: 0.0156\n",
            "rgb4fold\n",
            "Entreno 19\n",
            "(17, 0.9156249761581421)\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 361s 4s/step - loss: 0.4075 - accuracy: 0.8973\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 563s 7s/step - loss: 5.9080 - accuracy: 0.3313\n",
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 1026s 13s/step - loss: 30.7516 - accuracy: 0.0371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdsrVYT9aD5H",
        "outputId": "7a4d73e6-5003-4fef-d75b-699ad04d7b50"
      },
      "source": [
        "gen = SequenceGenerator(path, test, 32, 128, 128, 3)    \n",
        "eval_dict = model.evaluate(\n",
        "      gen\n",
        "    , steps=int(len(test)/32)\n",
        "    , batch_size=32\n",
        "    , verbose = 1\n",
        "    , return_dict=True\n",
        "    ,use_multiprocessing=True\n",
        "    ,workers=4\n",
        "  )\n",
        "#mean_acc[model_type].append(eval_dict['accuracy'])\n",
        "mean_acc[num_frames].append(eval_dict['accuracy'])\n",
        "with open(os.path.join(expFolder, f'evaluation_epoch{best_epoch}_frames.pickle'), 'wb') as pkl:\n",
        "  pickle.dump(eval_dict, pkl)    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
            "80/80 [==============================] - 993s 12s/step - loss: 0.0582 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaJU-sV2aKzs",
        "outputId": "1021a996-57fa-43f9-fa0b-8749dc67f76b"
      },
      "source": [
        "mean_acc['8'].pop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02187499962747097"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxDkPdMfaMp-",
        "outputId": "60dd4333-5a55-4d4b-bc66-530e1d83ae5f"
      },
      "source": [
        "mean_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'16': [0.2789062559604645, 0.3910156190395355, 0.42851561307907104],\n",
              " '32': [0.9984375238418579, 0.994921863079071, 1.0],\n",
              " '8': [0.03750000149011612, 0.02187499962747097, 0.05546874925494194]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXSdiZrvuuBx"
      },
      "source": [
        "with open(os.path.join(baseFolder, f'tests.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(mean_acc, pkl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biFw7pfp3QpX"
      },
      "source": [
        "with open(os.path.join(baseFolder, f'bestEpochs.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(best_epochs, pkl)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_YwZ-1z3S4U"
      },
      "source": [
        "with open(os.path.join(expFolder, f'evaluation_epoch{best_epoch}_frames.pickle'), 'wb') as pkl:\n",
        "      pickle.dump(eval_dict, pkl)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4d5yqqc3WTV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}